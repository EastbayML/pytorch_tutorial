{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_tutorial1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sSpnoeWNEkBR"
      ],
      "authorship_tag": "ABX9TyNZu7jWHHVSt+QtPXxRdvuY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EastbayML/pytorch_tutorial/blob/master/pytorch_tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri5PDh4yRTab",
        "colab_type": "text"
      },
      "source": [
        "# Setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1mkFeIUIIXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML\n",
        "import copy\n",
        "from IPython.core.debugger import set_trace\n",
        "from IPython.display import clear_output\n",
        "import math\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from functools import partial\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5i_26lOZ3nU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "East Bay Machine Learning pytorch tutorial series.\n",
        "\n",
        "---\n",
        "\n",
        "We will use this notebook as the launch point for our pytorch tutorial series.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxpEAveycl2y",
        "colab_type": "text"
      },
      "source": [
        "We willl used a google doc as a shared online clipboard. \n",
        "https://docs.google.com/document/d/1N-5Ue0rk7g8CImayet-cgnaCkHJzXky7aeHrwUocXEM/edit?usp=sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzWZwUrJBVxC",
        "colab_type": "text"
      },
      "source": [
        "#Agenda\n",
        "We will be starting with the pytorch.org/tutorials 60 minute blitz and bounce back here occasionally\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
        "\n",
        "Then we will do a brief exploration of deep learning using pytorch autograd\n",
        "\n",
        "Then we will jump into pytorch.nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FyvEMbbF1Wr",
        "colab_type": "text"
      },
      "source": [
        "# but first lets start with a little bit of overview (borrowed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZdY1Nw2zGE5X"
      },
      "source": [
        "PyTorch vs TensorFlow\n",
        "* Dynamic vs Static: Though both PyTorch and TensorFlow work on tensors, the primary difference between PyTorch and Tensorflow is that while PyTorch uses dynamic computation graphs, TensorFlow uses static computation graphs. That being said, with the release of TensorFlow 2.0 there has been a major shift towards eager execution, and away from static graph computation. Eager execution in TensorFlow 2.0 evaluates operations immediately, without building graphs.\n",
        "* Data Parallelism: PyTorch uses asynchronous execution of Python to implement data parallelism, but with TensorFlow this is not the case. With TensorFlow you need to manually configure every operation for data parallelism.\n",
        "* Visualization Support: TensorFlow has a very good visualization library called TensorBoard. This visualization support helps developers to track the model training process nicely. PyTorch initially had a visualization library called Visdom, but has since provided full support for TensorBoard as well. PyTorch users can utilize TensorBoard to log PyTorch models and metrics within the TensorBoard UI. Scalars, images, histograms, graphs, and embedding visualizations are all supported for PyTorch models and tensors.\n",
        "* Model Deployment: TensorFlow has great support for deploying models using a framework called TensorFlow serving. It is a framework that uses REST Client API for using the model for prediction once deployed. On the other hand, PyTorch does not provide a framework like serving to deploy models onto the web using REST Client.\n",
        "https://hub.packtpub.com/what-is-pytorch-and-how-does-it-work/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XQeurH5mGE5g"
      },
      "source": [
        "# why pytorch\n",
        "\n",
        "\n",
        "https://blog.paperspace.com/why-use-pytorch-deep-learning-framework/\n",
        "\n",
        "In the recent Kaggle competition, PyTorch library was used by nearly all of the top 10 finishers.\n",
        "\n",
        "Some of the key highlights of PyTorch includes:\n",
        "\n",
        "* Simple Interface: It offers easy to use API, thus it is very simple to operate and run like Python.\n",
        "* Pythonic in nature: This library, being Pythonic, smoothly integrates with the Python data science stack. Thus it can leverage all the services and functionalities offered by the Python environment.\n",
        "* Computational graphs: In addition to this, PyTorch provides an excellent platform which offers dynamic computational graphs, thus you can change them during runtime. This is highly useful when you have no idea how much memory will be required for creating a neural network model.\n",
        "\n",
        "* torch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
        "* nn.Module - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
        "* nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
        "* autograd.Function - Implements forward and backward definitions of an autograd operation. Every Tensor operation, creates at least a single Function node, that connects to functions that created a Tensor and encodes its history.\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2019/09/introduction-to-pytorch-from-scratch/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thf1ZedbiuBR",
        "colab_type": "text"
      },
      "source": [
        "# Select a device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N18NriLgiqjs",
        "colab_type": "code",
        "outputId": "ef8106d5-89e2-45cc-c470-f55e64c79cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfKyRKincd7P",
        "colab_type": "text"
      },
      "source": [
        "#ENABLE GPU\n",
        "To enable GPU hardware accelerator, just go to Runtime -> Change runtime type -> Hardware accelerator -> GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvf8AG2BbTkx",
        "colab_type": "code",
        "outputId": "63a2dca4-a23f-43e3-8312-856c122f2a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"Device is \",device)\n",
        "x = torch.empty(5, 3)\n",
        "y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
        "x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
        "z = x + y\n",
        "print(z)\n",
        "w=z.to(\"cpu\", torch.double)\n",
        "\n",
        "print(\"x is cuda \",x.is_cuda)\n",
        "print(\"y is cuda \",y.is_cuda)\n",
        "print(\"z is cuda \",z.is_cuda)\n",
        "print(\"w is cuda \",w.is_cuda)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device is  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T97fVTZ4RXWN",
        "colab_type": "text"
      },
      "source": [
        "# Benchmarks\n",
        "Let's compare execution times for numpy, torch cpu and torch gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0qwPm1PUNrD",
        "colab_type": "text"
      },
      "source": [
        "First setup a convenience function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqK5r0DmRbLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import timeit\n",
        "\n",
        "\n",
        "def benchmark(stmt,*args):\n",
        "  number=100\n",
        "  print(\"Numpy time   {:.6f}s\".format(timeit.timeit(partial(stmt,np,*args),number=1000)/number))\n",
        "  targs=[torch.tensor(arg,device='cpu') for arg in args]\n",
        "  print(\"pytorch time {:.6f}s\".format(timeit.timeit(partial(stmt,torch,*targs),number=1000)/number))\n",
        "  targs=[arg.to(torch.device('cuda')) for arg in targs]\n",
        "  print(\"gpu time     {:.6f}s\".format(timeit.timeit(partial(stmt,torch,*targs),number=1000)/number))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTOLawqxcsch",
        "colab_type": "text"
      },
      "source": [
        "Next write a simple function to be benchmarked for all three types of variables. Notice that the module name 'np' as the first argument will be replaced with 'torch' for the torch benchmarks. The remaining arguments are expected to be numpy tensors and will be converted to torch tensors for the second benchmark and moved to the gpu for the third benchmark.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYmcxLWfTi00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stmt(lib,a,b):\n",
        "  a=lib.sin(a)\n",
        "  b=lib.sqrt(b)\n",
        "  c=a+b\n",
        "\n",
        "\n",
        "a=np.zeros((300,500))\n",
        "b=np.zeros((300,500))\n",
        "\n",
        "benchmark(stmt,a,b)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxvrM5Pe-_Wx",
        "colab_type": "text"
      },
      "source": [
        "# Numpy shares memory with torch in cpu (but not cuda)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS_mUcz6_CUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=torch.zeros((1,2))\n",
        "c=a.cuda()\n",
        "b=a.numpy()\n",
        "a[0,1]=1\n",
        "print(\"a \",a)\n",
        "print(\"b \",b)\n",
        "print(\"c \",c)\n",
        "try:\n",
        "  d=c.numpy()\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6TxrK7Naw5s",
        "colab_type": "text"
      },
      "source": [
        "# Now you do one. \n",
        "Do a benchmark of an interesting operation and post it on the shared clipboard. Most math ops are supported. \n",
        "\n",
        "https://pytorch.org/docs/stable/tensors.html\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.functional.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r98kQuar6Yy",
        "colab_type": "text"
      },
      "source": [
        "# Plotting with matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "08RTGn_xE3MP",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        " \n",
        "x  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "y1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\n",
        "y2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\n",
        "plt.plot(x, y1, label=\"line L\")\n",
        "plt.plot(x, y2, label=\"line H\")\n",
        "plt.plot()\n",
        "plt.xlabel(\"x axis\")\n",
        "plt.ylabel(\"y axis\")\n",
        "plt.title(\"Line Graph Example\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToITShM5HrmS",
        "colab_type": "text"
      },
      "source": [
        "# BTW jupyter had as a debugger\n",
        "\n",
        "Remove the two comment #s and give it a try\n",
        "\n",
        "%debug\n",
        "\n",
        "set_trace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-l8ncGgfHr73",
        "colab": {}
      },
      "source": [
        "#%debug\n",
        "def fac(x):\n",
        "  if x<=1:\n",
        "    return 1\n",
        "  #set_trace()\n",
        "  return x*fac(x-1)\n",
        "fac(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHDsHqwZrj6Z",
        "colab_type": "text"
      },
      "source": [
        "# Graph vizualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMb5DNopqJ0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxu3FaMgdSDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchviz import make_dot\n",
        "\n",
        "x = torch.empty(5, 3,requires_grad=True)\n",
        "y = torch.empty(5, 3,requires_grad=True)\n",
        "z = x + y * 10\n",
        "for l in range(2):\n",
        "  w=torch.randn((3,3),requires_grad=True)\n",
        "  z=z.mm(w)+z\n",
        "make_dot(z)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RMVvAPta_Ab",
        "colab_type": "text"
      },
      "source": [
        "# Now you do one\n",
        "* Create an interesting pytorch tensor and post the visualization of the call graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwarXGgQ9szB",
        "colab_type": "text"
      },
      "source": [
        "# Autograd - Try it out.\n",
        "\n",
        "[Autograd 60 min blitz](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)\n",
        "\n",
        "[Test a torch function and verify the gradient is correct.](https://pytorch.org/docs/stable/torch.html#math-operations)\n",
        "\n",
        "[Or use the numerical gradient checking to verify a gradient] (https://pytorch.org/docs/stable/autograd.html#numerical-gradient-checking)\n",
        "\n",
        "\n",
        "\n",
        "https://pytorch.org/docs/stable/autograd.html\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/autograd.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XM__RNECiBP",
        "colab_type": "text"
      },
      "source": [
        "# Defining new autograd functions\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OuU_20ZBx93",
        "colab_type": "text"
      },
      "source": [
        "# Deep learning\n",
        "This is a brief introduction to deep learning using pytorch the autograd feature. While this is a very simple model, the concepts of optimizing a parameterized function are the same as what is used in more complex models.\n",
        "\n",
        "Once we have a dataset, a model and a loss function defined, we use the autograd feature to move the parameters in a way that reduces the loss for a given training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUIdCuO8f0MM",
        "colab_type": "text"
      },
      "source": [
        "# Gradient descent linear regresion example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3CWoUqcjDdk",
        "colab_type": "text"
      },
      "source": [
        "first create a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCY5JTIkCwF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# just run on cpu for now\n",
        "device=\"cpu\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGG5dw56foxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Generation\n",
        "def dataset_from_function_with_noise(f,npoints=100,noise=0.1):\n",
        "  torch.manual_seed(42)\n",
        "  x_train_tensor = torch.rand(int(npoints*.8),device=device)\n",
        "  y_train_tensor = f(x_train_tensor)\n",
        "  y_train_tensor += torch.randn(y_train_tensor.shape)*noise\n",
        "  return x_train_tensor,y_train_tensor\n",
        "\n",
        "x_train_tensor,y_train_tensor = dataset_from_function_with_noise(lambda x:1 + 2 * x  )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j4DvoU2uWmZ",
        "colab_type": "text"
      },
      "source": [
        "Now define function that we would like to fit to this data.  We will start with a linear function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp3qxxZRHwiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(x):\n",
        "  return model.a+model.b*x\n",
        "model.a=torch.randn(1)\n",
        "model.b=torch.randn(1)\n",
        "\n",
        "print(vars(model))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW5kMBc8bVC3",
        "colab_type": "text"
      },
      "source": [
        "# Define gradient descent algorithm.\n",
        "The gradient descent starts with randomize parameters. Loops through a number of epochs and each loop computes the loss function and the gradient of the loss function with respect to the parameters. Then it adjusts the parameters in a direction that minimizes the loss. It does that by  multiplying the mean of the gradient of the loss function, times a learning rate hyperparameter and adds that to the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e-YfKIXW0WeU",
        "colab": {}
      },
      "source": [
        "\n",
        "def initializer(model):\n",
        "  for param in model.params if hasattr(model,'params') else vars(model).values():\n",
        "    param.requires_grad=False\n",
        "    torch.randn(param.shape,out=param,requires_grad=True)\n",
        "    \n",
        "def gd(model,x_train_tensor,y_train_tensor,lr=1e-1,n_epochs=1000,hook=lambda m,x,y:None,\n",
        "       initialize=initializer):\n",
        "  initialize(model)\n",
        "  hook(model,x_train_tensor,y_train_tensor)\n",
        "  for epoch in range(n_epochs): \n",
        "    single_step(model,x_train_tensor,y_train_tensor,lr=lr)\n",
        "    hook(model,x_train_tensor,y_train_tensor)\n",
        "\n",
        "def single_step(model,x_tensor,y_tensor,lr=1e-1,hook=None):\n",
        "  yhat = model(x_tensor)\n",
        "  assert yhat.requires_grad,\"Model forward lost the requires gradient attribute\"\n",
        "  error = y_tensor - yhat\n",
        "  loss = (error ** 2).mean()\n",
        "\n",
        "  # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
        "  with torch.no_grad():\n",
        "    for param in model.params if hasattr(model,'params') else vars(model).values():\n",
        "      try:\n",
        "        param.grad.zero_()\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  # UPDATING the parameters \n",
        "  # We need to use NO_GRAD to keep the update out of the gradient computation\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for param in model.params if hasattr(model,'params') else vars(model).values():\n",
        "      param -= lr * param.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mto0L1Q3g7YG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now test it\n",
        "gd(model,x_train_tensor,y_train_tensor)\n",
        "print(vars(model))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6rrDL7mcWgf",
        "colab_type": "text"
      },
      "source": [
        "# Alternative model\n",
        "The model we define above is a function with attributes.  Functions are objects in python and this is equivalent to defining an object with a __call__ method and attributes. If you prefer using classes, this is the equivalent definition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8tinpbCHMy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.a=torch.randn(1)\n",
        "    self.b=torch.randn(1)\n",
        "\n",
        "  def __call__(self,x):\n",
        "    return self.a+self.b*x\n",
        "\n",
        "model=Model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2alOqvCtjegJ",
        "colab_type": "text"
      },
      "source": [
        "# visualizing GD\n",
        "Let's see what the solution looks like as the gradient descent optimizes the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNh4Ez0Pn4Cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng=torch.linspace(0,1,10)\n",
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "def movie_hook(model,x_tensor,y_tensor):\n",
        "  scat=plt.scatter(x_tensor.cpu(),y_tensor.cpu(),c='b')\n",
        "  l1,=plt.plot(list(rng),list(model(rng)),c='r')\n",
        "  artists.append([scat,l1])\n",
        "\n",
        "def plot_hook(model,x_tensor,y_tensor):\n",
        "  global rng\n",
        "  plt.scatter(x_tensor.cpu(),y_tensor.cpu(),c='b')\n",
        "  plt.plot(list(rng),list(model(rng)),c='r')\n",
        "  plt.pause(0.5)\n",
        "  clear_output() \n",
        "    \n",
        "torch.manual_seed(42)\n",
        "gd(model,x_train_tensor,y_train_tensor,hook=movie_hook,n_epochs=100)\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=50, blit=True,repeat_delay=1000).to_html5_video()) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXwXWA6DLK1f",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing the loss function\n",
        "Lets look at the topology of the loss function. Rember that the model output is a function of the input and a set of tunable parameters (a&b).  In the previous graph we plotted the output for a range of inputs for a set of paramters. We varied the parameters over time and visualized the changes to the input/output function.  \n",
        "\n",
        "In this case we will vary the two parameters a&b over a range and vizualize the mean error of the model for a range of X.  So the plot is a 2d surface, where color represents the log(mean(Y - model(X,(a,b))). \n",
        "\n",
        "Notice that the shape is has a sharp gradient in some directions and a low gradient in other directions. This is why we observe that we quickly adjust the parameters at first, but then slowly fine tune the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl5JLcpzLO5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "arng=torch.linspace(0,2,100)\n",
        "brng=torch.linspace(0,4,100)\n",
        "thetagrid=torch.cat([x.unsqueeze(-1) for x in torch.meshgrid(arng,brng)],dim=-1)\n",
        "\n",
        "def lossf(model,X,Y,theta):\n",
        "    saveit=model.a,model.b\n",
        "    model.a,model.b=theta\n",
        "    yhat = model(X)\n",
        "    error = Y - yhat\n",
        "    loss = (error ** 2).mean()\n",
        "    model.a,model.b=saveit\n",
        "    return math.log(loss)\n",
        "\n",
        "def torchmap(func,grid,argdim=1):\n",
        "  global thetagrid\n",
        "  if len(grid.shape)==argdim+1:\n",
        "    return torch.tensor([func(t) for t in grid])\n",
        "  else:\n",
        "    t= torch.cat([torchmap(func,t) for t in thetagrid])\n",
        "    return t.reshape(thetagrid.shape[:-1])\n",
        "\n",
        "ax.set_aspect('equal', 'datalim')\n",
        "plt.pcolor(brng,arng,torchmap(partial(lossf,model,x_train_tensor,y_train_tensor),thetagrid))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TD9mkYf7-kG",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing GD trajectory over the loss surface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Da7NEZIwjK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trajectory_hook(artists,model,grid,X,Y):\n",
        "  artists.clear()\n",
        "  trajectory=[]\n",
        "  #pre compute the background to save time\n",
        "  background=plt.pcolor(grid[0,:,1],grid[:,0,0],torchmap(partial(lossf,model,X,Y),grid))\n",
        "  def func(model,X,Y):\n",
        "    trajectory.append((model.b.item(),model.a.item()))  \n",
        "    l1,=plt.plot(*zip(*trajectory) ,c='r')\n",
        "    artists.append([l1,background])\n",
        "\n",
        "  return func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDhjVWcnfKBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_aspect('equal', 'datalim')\n",
        "torch.manual_seed(42)\n",
        "gd(model,x_train_tensor,y_train_tensor,hook=trajectory_hook(artists,model,thetagrid,x_train_tensor,y_train_tensor),n_epochs=200)\n",
        "print(model.a,model.b)\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=50, blit=True,repeat_delay=1000).to_html5_video()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ2soqzgx1_j",
        "colab_type": "text"
      },
      "source": [
        "# learning rate\n",
        "What happens if we increase the learning rate?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCU2aOXlwyOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_aspect('equal', 'datalim')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "gd(model,x_train_tensor,y_train_tensor,hook=trajectory_hook(artists,model,thetagrid,x_train_tensor,y_train_tensor),n_epochs=200,lr=6e-1)\n",
        "\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=50, blit=True,repeat_delay=1000).to_html5_video()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExqQfFA8TGdr",
        "colab_type": "text"
      },
      "source": [
        "# Reparameterization\n",
        "\n",
        "The model parameters can be transformed to make the loss function more bowl shaped and less taco shaped.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y1BDGOeTErU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modeln(x):\n",
        "  a=modeln.a+1-modeln.b*2\n",
        "  b=modeln.b*4\n",
        "  return a+b*x\n",
        "modeln.a=torch.randn(1)\n",
        "modeln.b=torch.randn(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyBODdOpTXa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.set_aspect('equal', 'datalim') \n",
        "\n",
        "\n",
        "plt.pcolor(brng,arng,torchmap(partial(lossf,modeln,x_train_tensor,y_train_tensor),thetagrid))\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fsoImM_2Lm1",
        "colab_type": "text"
      },
      "source": [
        "Now lets watch the gradient descent on the new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG3EPP-qCFEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fixed_initializer(model,params=None):\n",
        "  for param,value in zip((model.params if hasattr(model,'params') else vars(model).values()),params):\n",
        "    with torch.no_grad():\n",
        "      param.fill_(value)\n",
        "      param.requires_grad=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVBiHzkmC1dJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_aspect('equal', 'datalim')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "gd(modeln,x_train_tensor,y_train_tensor,hook=trajectory_hook(artists,modeln,thetagrid,x_train_tensor,y_train_tensor),n_epochs=20,\n",
        "  initialize=partial(fixed_initializer,params=[0,0]))\n",
        "print(model.a,model.b)\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=50, blit=True,repeat_delay=1000).to_html5_video()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b0RzPWy2esn",
        "colab_type": "text"
      },
      "source": [
        "Now lets watch that in the input and output space (X and Y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg9XSSsu2dmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng=torch.linspace(0,1,10)\n",
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "gd(modeln,x_train_tensor,y_train_tensor,hook=movie_hook,n_epochs=20,initialize=partial(fixed_initializer,params=[0,0]))\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=50, blit=True,repeat_delay=1000).to_html5_video()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3wPXF23_vDB",
        "colab_type": "text"
      },
      "source": [
        "But does this work for other linear datasets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15t0HJ9h5C3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "x2_train_tensor,y2_train_tensor = dataset_from_function_with_noise(lambda x:4 - 1 * x  )\n",
        "\n",
        "\n",
        "rng=torch.linspace(0,1,10)\n",
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "gd(modeln,x2_train_tensor,y2_train_tensor,hook=movie_hook,n_epochs=10)\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=150, blit=True,repeat_delay=1000).to_html5_video()) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nH-Dn6zQpuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.set_aspect('equal', 'datalim') \n",
        "artists=[]\n",
        "torch.manual_seed(60)\n",
        "gd(modeln,x2_train_tensor,y2_train_tensor,hook=trajectory_hook(artists,modeln,thetagrid,x2_train_tensor,y2_train_tensor),n_epochs=10)\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=150, blit=True,repeat_delay=1000).to_html5_video()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcJkfs426-23",
        "colab_type": "text"
      },
      "source": [
        "# Stochastic Gradient Descent\n",
        "Technically what we have done so far is not stochastic gradient descent because for every a&b we evaluate the loss function for all values of the training data set.  Stochastic Gradient Descent computes the loss function for a subset of the data and then updates the parameters based on the gradient. So let't define sgd() and see if we get the same answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNHbX1ra7CO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd(model,x_train_tensor,y_train_tensor,batchsize=32,lr=1e-1,n_epochs=100,hook=lambda m,x,y:None,\n",
        "        initialize=initializer):\n",
        "  initialize(model)\n",
        "  for epoch in range(n_epochs):\n",
        "    for batch in range(int(n_epochs/batchsize)+1):\n",
        "      rows = torch.randperm(x_train_tensor.shape[0])[:batchsize]\n",
        "      single_step(model,x_train_tensor[rows],y_train_tensor[rows])\n",
        "    hook(model,x_train_tensor,y_train_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA5HEv5Rirbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd(model,x_train_tensor,y_train_tensor)\n",
        "print(vars(model))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7jX9gdnX4-w",
        "colab_type": "text"
      },
      "source": [
        "# now lets visualize SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCysPqyrX-7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "rng=torch.linspace(0,1,10)\n",
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "torch.manual_seed(42)\n",
        "sgd(model,x_train_tensor,y_train_tensor,hook=movie_hook,n_epochs=100,lr=1e-1)\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=50, blit=True,repeat_delay=1000).to_html5_video()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slFSwSpAywsF",
        "colab_type": "text"
      },
      "source": [
        "Now lets visualize SGD in parameter space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgtmEofMyecT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_aspect('equal', 'datalim')\n",
        "torch.manual_seed(42)\n",
        "sgd(model,x_train_tensor,y_train_tensor,hook=trajectory_hook(artists,model,thetagrid,x_train_tensor,y_train_tensor),n_epochs=200)\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=50, blit=True,repeat_delay=1000).to_html5_video()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAK6ccCa86dR",
        "colab_type": "text"
      },
      "source": [
        "# How do minibatches change the loss function surface?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgGNdV7V9Xxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.text import Text \n",
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_aspect('equal', 'datalim')\n",
        "model.a=1\n",
        "model.b=2\n",
        "batchsize=128\n",
        "for batchsize in [16,32,64,128,256]:\n",
        "  for batch in range(20):\n",
        "    rows = torch.randperm(x_train_tensor.shape[0])[:batchsize]\n",
        "    txt = plt.text(0,0,str(batchsize) )\n",
        "    background=plt.pcolor(thetagrid[0,:,1],thetagrid[:,0,0],torchmap(partial(lossf,model,x_train_tensor[rows],y_train_tensor[rows]),thetagrid))\n",
        "    artists.append([background,txt])\n",
        "\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=50, blit=True,repeat_delay=1000).to_html5_video()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyfutKjdY9xM",
        "colab_type": "text"
      },
      "source": [
        "# deep learning models \n",
        "often use layers of linear and nonlinear functions with hidden activation layers. Lets try that now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4a9vPPF38Vo",
        "colab_type": "text"
      },
      "source": [
        "we need to change some of our utility functions to work with tensors instead of scalars\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEaOkaxOnZh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Generation\n",
        "def vector_dataset_from_function_with_noise(f,npoints=100,noise=0.1):\n",
        "  torch.manual_seed(42)\n",
        "  x_train_tensor = torch.rand((int(npoints*.8),1),device=device)\n",
        "  y_train_tensor = f(x_train_tensor)\n",
        "  y_train_tensor += torch.randn(y_train_tensor.shape)*noise\n",
        "  return x_train_tensor,y_train_tensor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLm7WlwtnrhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xv_train_tensor,yv_train_tensor = vector_dataset_from_function_with_noise(lambda x:1 + 4 * (x-0.5)**2,noise=0.01 ,npoints=100 )\n",
        "plt.scatter(xv_train_tensor,yv_train_tensor,c='b')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbqs0fq0RfY6",
        "colab_type": "text"
      },
      "source": [
        "# Our deep model\n",
        "Now define a model with multiple weighted linear segments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDvLZzEkZRHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelv(x):\n",
        "  if len(x.shape)==1:\n",
        "    x=x.unsqueeze(-1)\n",
        "  modelv.x=x\n",
        "  modelv.h1 = modelv.w1.mm(x.T).T +modelv.b1\n",
        "  modelv.a1 = torch.relu(modelv.h1)\n",
        "  modelv.h2 = modelv.w2.mm(modelv.a1.T).T+modelv.b2\n",
        "  return modelv.h2\n",
        "hl=10\n",
        "modelv.w1=torch.randn((hl,1))\n",
        "modelv.b1=torch.randn((hl))\n",
        "modelv.w2=torch.randn((1,hl))\n",
        "modelv.b2=torch.randn((1,))\n",
        "modelv.params=[modelv.w1,modelv.b1,modelv.w2,modelv.b2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ6349yjauid",
        "colab_type": "text"
      },
      "source": [
        "Now train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHhr0hfoaxpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "rng=torch.linspace(0,1,10)\n",
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "torch.manual_seed(42)  # converged\n",
        "#torch.manual_seed(60)   # doesnt converge\n",
        "sgd(modelv,xv_train_tensor,yv_train_tensor,hook=movie_hook,n_epochs=300,lr=1e-2)\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=50, blit=True,repeat_delay=1000).to_html5_video()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbB4LMmJ5yj1",
        "colab_type": "text"
      },
      "source": [
        "# initialization matters\n",
        "In my tests, the sgd only converges occasionally. I assume this is because of the random parameter values.\n",
        "\n",
        "Also, I noticed that it will not converge with gd()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXl0bMgbI3E4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "with torch.no_grad():\n",
        "  initializer(modelv)\n",
        "  print(torch.mean(modelv.w1),torch.mean(modelv.b1),torch.mean(modelv.w2))\n",
        "  initializer(modelv)\n",
        "  print(torch.mean(modelv.w1),torch.mean(modelv.b1),torch.mean(modelv.w2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7iM85dIsLAv",
        "colab_type": "text"
      },
      "source": [
        "# Parameter trajectory\n",
        "Because the parameter space is high dimensionality, we cannot plot parameter trajectory over the loss surface, but we can plot the trajectory in time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z20eeblSJrnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "def hooks(hs,m,x,y):\n",
        "  for hook in hs:\n",
        "    hook(m,x,y)\n",
        "\n",
        "def recording_hook(md,m,x,y):\n",
        "  for name,value in vars(m).items():\n",
        "    if torch.is_tensor(value):\n",
        "      md[name].append(value.clone())\n",
        "      if value.requires_grad and value.grad is not None:\n",
        "        md[name+\".grad\"].append(value.grad.clone())\n",
        "\n",
        "def recorder2tensor(recorder):\n",
        "  ret={}\n",
        "  for name in recorder:\n",
        "    tl=[t.detach().numpy() for t in recorder[name]]\n",
        "    if len(tl[0].shape)==2:\n",
        "      if tl[0].shape[1]==1:\n",
        "        v=np.hstack(tl)  \n",
        "      elif tl[0].shape[0]==1:\n",
        "        v=np.hstack([t.detach().numpy().T for t in recorder[name]])\n",
        "      else:\n",
        "        v=np.hstack([t.detach().numpy().T for t in recorder[name]])[:,::10]     \n",
        "    elif len(tl[0].shape)==1:\n",
        "      v=np.hstack([np.expand_dims(t.detach().numpy(), axis=-1) for t in recorder[name]])\n",
        "    else:\n",
        "      assert \"unexpected shape\".format(tl[0].shape) \n",
        "    ret[name]=v\n",
        "  return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F46WWdwV-4wO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "rng=torch.linspace(0,1,10)\n",
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "torch.manual_seed(42)  # converged\n",
        "#torch.manual_seed(60)   # doesnt converge\n",
        "recorder=defaultdict(list)\n",
        "sgd(modelv,xv_train_tensor,yv_train_tensor,hook=partial(hooks,[movie_hook,partial(recording_hook,recorder)]),n_epochs=300,lr=1e-2)\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=50, blit=True,repeat_delay=1000).to_html5_video()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onZg1fqBTr4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "results=recorder2tensor(recorder)\n",
        "def pltresults(results):\n",
        "  fig= plt.figure(figsize=(20,10))\n",
        "  for idx,trace in enumerate( ['w1','w2','b1','b2','w1.grad','w2.grad','b1.grad','b2.grad']):\n",
        "    c='r' if '2' in trace else 'b'\n",
        "    plt.subplot(2,4,idx+1)\n",
        "    for l in results[trace]:\n",
        "      plt.title(trace)\n",
        "      plt.plot(l)\n",
        "  plt.pause(1)\n",
        "\n",
        "pltresults(results)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9TPz0a8LkBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng=torch.linspace(0,1,10)\n",
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "torch.manual_seed(60)   # doesnt converge\n",
        "recorder=defaultdict(list)\n",
        "sgd(modelv,xv_train_tensor,yv_train_tensor,hook=partial(hooks,[movie_hook,partial(recording_hook,recorder)]),n_epochs=300,lr=1e-2)\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=50, blit=True,repeat_delay=1000).to_html5_video()) \n",
        "\n",
        "results=recorder2tensor(recorder)\n",
        "pltresults(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEjzY1QpMHHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelv(x):\n",
        "  if len(x.shape)==1:\n",
        "    x=x.unsqueeze(-1)\n",
        "  modelv.x=x\n",
        "  modelv.h1 = modelv.w1.mm(x.T).T +modelv.b1\n",
        "  modelv.a1 = torch.functional.F.leaky_relu(modelv.h1,negative_slope=0.1)\n",
        "  modelv.h2 = modelv.w2.mm(modelv.a1.T).T+modelv.b2\n",
        "  return modelv.h2\n",
        "hl=10\n",
        "modelv.w1=torch.randn((hl,1))\n",
        "modelv.b1=torch.randn((hl))\n",
        "modelv.w2=torch.randn((1,hl))\n",
        "modelv.b2=torch.randn((1,))\n",
        "modelv.params=[modelv.w1,modelv.b1,modelv.w2,modelv.b2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlEGXc8aMM2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng=torch.linspace(0,1,10)\n",
        "artists=[]\n",
        "fig, ax = plt.subplots()\n",
        "torch.manual_seed(60)   # doesnt converge with relu\n",
        "recorder=defaultdict(list)\n",
        "plt.ylim(0,3)\n",
        "sgd(modelv,xv_train_tensor,yv_train_tensor,hook=partial(hooks,[movie_hook,partial(recording_hook,recorder)]),n_epochs=200,lr=1e-2)\n",
        "HTML(animation.ArtistAnimation(fig, artists,interval=50, blit=True,repeat_delay=1000).to_html5_video()) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnoDuzCwOD8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results=recorder2tensor(recorder)\n",
        "pltresults(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brqvwbmBYSG0",
        "colab_type": "text"
      },
      "source": [
        "# Question\n",
        "\n",
        "Why doesnt the linear model above use all of the hidden layer activations to make a closer fit?\n",
        "\n",
        "* I tried 3000 epoch and it never improved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSpnoeWNEkBR",
        "colab_type": "text"
      },
      "source": [
        "## Links and stuff\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfgGC5OKm4kS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%debug\n",
        "def fac(x):\n",
        "  if x<=1:\n",
        "    return 1\n",
        "  #set_trace()\n",
        "  return x*fac(x-1)\n",
        "fac(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qofukqt1ESpg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "https://blog.paperspace.com/why-use-pytorch-deep-learning-framework/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o47PMk2HLG7g",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "https://hub.packtpub.com/what-is-pytorch-and-how-does-it-work/\n",
        "\n",
        "https://pytorch.org/docs/master/named_tensor.html\n",
        "https://en.wikipedia.org/wiki/Automatic_differentiation\n",
        "https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
        "https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py\n",
        "https://www.analyticsvidhya.com/blog/2019/09/introduction-to-pytorch-from-scratch/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b16YkOU2HORD",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/09/dynamic_graph.gif)"
      ]
    }
  ]
}